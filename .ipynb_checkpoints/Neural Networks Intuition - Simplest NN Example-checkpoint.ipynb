{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Networks Intuition - Simplest NN Example</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn about the simplest Neural Network. You'll learn basic notation, basic computations, and we'll train the network to learn how to compute the NOT operation (0 becomes 1 and 1 becomes 0). The goal is to get someone new to the subject to build intuition about Nerual Networks without having to hold giant matricies and perform linear algebra calculations in their head at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is a Neural Network</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a Neural Network (NN)? A NN is a computer program. Every program takes input, processes that input, and returns output. The input to a NN is a list of numbers (called features). These numbers can represent pixel color values, inches of rainfall, a rating on a scale of 1 to 10, or a 1 or 0 to encode a quality existing or not. In reality, you'll have many examples in a dataset, each with a different set of values for these inputs and a corresponding expected value for the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a NN is a list of numbers. When the NN is used to classify some input into a category, each output number is tied to a specific category and represents the likelihood of that category being the correct one (higher number means more likely). In reality, you'll be comparing the NN's output to an expected list of numbers for the given example in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as the processing of the input is concerned, it's best explained using an illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.otexts.org/sites/default/files/fpp/images/nnet1.png\" heigh=\"400\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review this simple NN diagram. It has four inputs (each will get a specific value for a given example from the dataset). Each of the lines from an input node to the Output node represents a weight value that will be multiplied by the input value (a NN is special because it learns these weight values from the examples). We're about to get math heavy, but it's pretty straight forward if you have any memory of algebra. So the way that the input is processed to construct the Output number is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Output = f(i_1 * w_1 + i_2 * w_2 + i_3 * w_3 + i_4 * w_4 + bias)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this calculation of the Output from the inputs, weights, and bias. The inputs, $i_1, i_2, i_3, i_4$, are just the numbers associated with each input feature for a given example in our dataset. The weights, $w_1, w_2, w_3, w_4$, are initialized randomly (but within some small bounds so we don't start with something that makes wild predections), and adjusted by comparing the actual Output value with the expected value for the given example. And the bias value is specific to the Output neuron. It's just an additional parameter that affects the Output value independant of the input values, and it will be modified similarly to the weight modifications (it's value will be learned). Finally, that \"weighted sum\", the sum of the weighted inputs and the bias, is passed to the activation function, $f$. This activation function is usually the sigmoid function or ReLU, both of which have non-linear properties, none of which matters for this discussion, but is important for complex networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a highly simplified version. In reality, there are often many more input nodes, several output nodes, and intermediate layers of nodes. This writeup will only focus on the simple version. In fact, we'll now introduce an even simpler version (the simplest!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Simplest NN</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Simplest NN Diagram.png\" height=\"500\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is so simple, can it even learn anything? Yes! We'll work on teaching it the NOT operation, which should give an Output of 0 for an input of 1, and an Output of 1 for an input of 0. First, let's look at how we calculate the Output. To make things even simpler, we're going to use the identity function, $f(x) = x$, for the activation function (the identity function does nothing, so it can be omitted):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of the discussion to come:\n",
    "1. Construct the perfect NOT NN.\n",
    "2. Discuss how we measure the \"wrongness\" of a model.\n",
    "3. Use Calculus to fix a wrong weight and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is so simple, can it even learn anything? Yes! We'll work on teaching it the NOT operation, which should give an Output of 0 for an input of 1, and an Output of 1 for an input of 0. First, let's look at how we calculate the Output. To make things even simpler, we're going to use the identity function, $f(x) = x$, for the activation function (the identity function does nothing, so it can be omitted):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Output = input * weight + bias\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider what the perfect weight and bias should be to turn this into the NOT operation. First, we want an input of 0 to give an Output of 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "input * weight + bias = 0 * weight + bias = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "bias = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want an input of 1 to give an Output of 0 (substituting 1 for the bias):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "input * weight + bias = 1 * weight + 1 = weight + 1 = 0 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "weight = -1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal network now looks like $Output = -1 * input + 1$ due to a weight value of $-1$ and a bias value of $1$. You can calculate in your head what happens for an input of $0$ or $1$ to see that it does compute the NOT operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to figure out ideal values because of the utter simplicity of the network, activation function, and NOT operation. However, we're going to pretend we don't know what values we want for the weight and bias, and learn them from examples. Each example will have two values, an input and an expected Output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our weight and bias to $0.5$. Now, to calculate the Output of our network, we'll use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Output = input * 0.5 + 0.5\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input is $0$, the Output will be $0 * 0.5 + 0.5 = 0.5$. When the input is $1$, the Output will be $1 * 0.5 + 0.5 = 1$. This is clearly not the NOT operation! We can tell, because our actual Output is not the same as our expected Output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we define a Cost function that measures how \"off\" our networks answers are for a given example. There are several Cost functions in use today, but we'll focus on a common one used in Statistics to measure error size. The Mean Squared Error (MSE) Cost function is (here $y$ is the expected value for a given example):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "C = \\frac{1}{2} (y - Output)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Cost function works as a Cost function, because if the actual Output and the expected Output are closer, the Cost is lower, and if they're farther apart, then the Cost becomes higher. Don't focus too much at this time on our choice of MSE as the Cost function, just convince yourself that it can be a Cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate how wrong our model is for each example. First we'll see how wrong we are for an input of $0$ (here $y$ will be $1$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "C = \\frac{1}{2} (1 - (0 * 0.5 + 0.5))^2 = \\frac{1}{2} (1 - 0.5)^2 = \\frac{1}{2} (.25) = .125\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model has a Cost of .125 (there are no units to this cost, it's just a number). Let's look at the cost for an input of $1$ (this time $y$ will be $0$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "C = \\frac{1}{2} (0 - (1 * 0.5 + 0.5))^2 = \\frac{1}{2} (0 - 1)^2 = \\frac{1}{2} (1) = .5\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is more wrong (has a higher cost) for an input of $1$ than it does for $0$. Let's notice some things. First, when the input was $0$, the weight value didn't contribute to the error at all (so our learning shouldn't affect it for that example). Also, that explains why the input of $1$ led to a higher Cost (the wrong weight and the wrong bias contirbuted to the Cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time for our model to learn from its mistakes! Now would be a good time to brush up on your Calculus, especially the Chain Rule. The videos by <a href=\"https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr\">3Blue1Brown</a> are really good for an intuitive sense of Calculus. If you are good with applying the chain rule to derivatives (<a href=\"https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction\">Khan Academy Chain Rule</a> is also good), the partial derivatives are even easier. For a partial derivative refresher, check out this <a href=\"https://www.youtube.com/watch?v=AXqhWeUEtQU\">Khan Academy Partial Derivatives</a> video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
